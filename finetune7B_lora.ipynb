{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4345522f",
   "metadata": {},
   "source": [
    "### Importing Libraries\n",
    "\n",
    "This cell imports the necessary libraries for the notebook, including PyTorch, the Hugging Face transformers library, the datasets library for loading datasets, and the peft library for applying LoRA configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6ee2d38-851c-42c2-851f-c58792578acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from collections import Counter\n",
    "import torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from peft import LoraConfig, TaskType\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7ab29c",
   "metadata": {},
   "source": [
    "### Setting Up Environment\n",
    "\n",
    "This cell clears the CUDA cache to manage memory effectively and sets up the Weights and Biases (WandB) project for logging training metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aae95839-17f3-476b-96c3-932c143d26b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure memory management first\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Define response template\n",
    "RESPONSE_TEMPLATE = \"<response>\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9c549d",
   "metadata": {},
   "source": [
    "## Loading and Preparing the Dataset\n",
    "\n",
    "Now, we extracts unique sentiment labels from the dataset and calculates their distribution, which is useful for understanding the dataset's balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a2985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get distinct classes from the dataset\n",
    "def get_distinct_classes(dataset):\n",
    "    # Extract all output labels from the training set\n",
    "    if isinstance(dataset, DatasetDict) and \"train\" in dataset:\n",
    "        labels = dataset[\"train\"][\"output\"]\n",
    "        \n",
    "        # Count unique labels and their distribution\n",
    "        label_counts = Counter(labels)\n",
    "        \n",
    "        # Get unique labels\n",
    "        unique_labels = set(labels)\n",
    "        \n",
    "        # Convert to percentage distribution\n",
    "        total_samples = len(labels)\n",
    "        \n",
    "        label_distribution = {label: count / total_samples * 100 for label, count in label_counts.items()}\n",
    "\n",
    "        # Print results\n",
    "        print(f\"Unique labels count: {len(label_counts)}\")\n",
    "        print(\"Label distribution:\", label_distribution)\n",
    "        \n",
    "        return unique_labels\n",
    "    \n",
    "    return set() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9be69b",
   "metadata": {},
   "source": [
    "This function loads the FinGPT sentiment analysis dataset, splits it into training and validation sets if necessary, and prints the dataset statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2721667-7a97-4338-9f5e-11b3ce05a057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and prepare the dataset\n",
    "def load_and_prepare_dataset():\n",
    "    print(\"Loading dataset...\")\n",
    "\n",
    "    # load FinGPT dataset\n",
    "    dataset = load_dataset(\"FinGPT/fingpt-sentiment-train\")\n",
    "    print(\"Successfully loaded FinGPT/fingpt-sentiment-train dataset\")\n",
    "    \n",
    "    # Split dataset into train and validation sets if only train split exists\n",
    "    if isinstance(dataset, DatasetDict) and \"train\" in dataset and \"validation\" not in dataset:\n",
    "        train_test_split = dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "        dataset = DatasetDict({\n",
    "            \"train\": train_test_split[\"train\"],\n",
    "            \"validation\": train_test_split[\"test\"]\n",
    "        })\n",
    "    \n",
    "    # Print dataset statistics\n",
    "    print(f\"Dataset loaded with {len(dataset['train'])} training samples and {len(dataset['validation'])} validation samples\")\n",
    "    \n",
    "    # Get distinct classes\n",
    "    distinct_classes = get_distinct_classes(dataset)\n",
    "    \n",
    "    return dataset, distinct_classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4111efb",
   "metadata": {},
   "source": [
    "## Loading the Model and Tokenizer\n",
    "\n",
    "This function loads the Mistral 7B model and its tokenizer, configuring them for use in the training process. It also handles special tokens required for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fa1c60b-5d0e-4e9b-a4e9-765eb6239e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name, cache_dir):\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    # Load tokenizer first with proper special tokens\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        cache_dir=cache_dir,\n",
    "        # padding_side=\"right\",\n",
    "        eos_token=\"<|im_end|>\",\n",
    "        bos_token=\"<s>\",\n",
    "        pad_token=\"<pad>\",\n",
    "        additional_special_tokens=[RESPONSE_TEMPLATE,\"<response|end>\"]\n",
    "    )\n",
    "    print(f\"Tokenizer loaded with vocab size: {len(tokenizer)}\")\n",
    "    \n",
    "    # Load model with quantization and device configuration\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        cache_dir=cache_dir,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        attn_implementation=\"flash_attention_2\"\n",
    "    )\n",
    "    print(\"Model loaded!\")\n",
    "    \n",
    "    \n",
    "    # Resize token embeddings to match tokenizer\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    print(\"Model and tokenizer loaded successfully\")\n",
    "\n",
    "    for p in model.parameters():\n",
    "        if not p.requires_grad:\n",
    "            p.data = p.to(torch.float16)\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3b417c",
   "metadata": {},
   "source": [
    "## Define LoRA Configuration\n",
    "\n",
    "This function sets up the LoRA configuration parameters, which dictate how the model will be fine-tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cd396ab-dac9-4539-b1e0-0b24d5b3c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lora_config():\n",
    "    # Define LoRA configuration\n",
    "    peft_config = LoraConfig(\n",
    "        r=16,                   # Rank dimension\n",
    "        lora_alpha=32,          # Alpha parameter for LoRA scaling\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\", \n",
    "            \"up_proj\",\n",
    "            \"down_proj\"\n",
    "        ],\n",
    "        lora_dropout=0.1,      # dropout for regularization\n",
    "        bias=\"none\",            # Don't train bias terms\n",
    "        task_type=TaskType.CAUSAL_LM  # Task type: causal language modeling\n",
    "    )\n",
    "    \n",
    "    return peft_config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba525433",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n",
    "This function formats the dataset for sentiment classification, creating input-output pairs that the model will use during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a9c3d3e-b5bb-4b47-95ff-2712f93c33e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(example):\n",
    "    \"\"\"Preprocess the data to format for sentiment classification.\"\"\"\n",
    "    # Format the prompt with sentiment label appended directly\n",
    "    RESPONSE_END_TEMPLATE=\"<response|end>\"\n",
    "    prompt = (\n",
    "        f\"Instruction: {example['instruction']}\\n\"\n",
    "        f\"Input: {example['input']}\\n\"\n",
    "        f\"{RESPONSE_TEMPLATE}{example['output']}{RESPONSE_END_TEMPLATE}\"\n",
    "    )\n",
    "    return {\"text\": prompt}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "808fd971-069b-4663-be21-15d91f36b479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_parameters(model):\n",
    "    \"\"\"Print parameter statistics for the model.\"\"\"\n",
    "    # Count total parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    # Count trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    # Compute percentage\n",
    "    trainable_percentage = (trainable_params / total_params) * 100\n",
    "    print(f\"Trainable params: {trainable_params:,} || All params: {total_params:,} || Trainable%: {trainable_percentage:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c60c8fd2-8b4b-4531-8af8-b85e2597292b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA with 1 available device(s)\n",
      "Device 0: NVIDIA RTX A6000\n",
      "Memory: 51.03 GB\n"
     ]
    }
   ],
   "source": [
    "# Check available CUDA devices\n",
    "if torch.cuda.is_available():\n",
    "    device_count = torch.cuda.device_count()\n",
    "    print(f\"Using CUDA with {device_count} available device(s)\")\n",
    "    for i in range(device_count):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e172923a-3bde-47aa-8ac3-1ac618d2c06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Successfully loaded FinGPT/fingpt-sentiment-train dataset\n",
      "Dataset loaded with 69094 training samples and 7678 validation samples\n",
      "Unique labels count: 9\n",
      "Label distribution: {'neutral': 38.078559643384374, 'strong positive': 0.2807769126118042, 'mildly positive': 3.311430804411382, 'moderately positive': 8.045561119634122, 'positive': 28.16597678524908, 'negative': 15.244449590413062, 'mildly negative': 2.7484296755145166, 'moderately negative': 3.844038556169856, 'strong negative': 0.2807769126118042}\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'output', 'instruction'],\n",
      "        num_rows: 69094\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input', 'output', 'instruction'],\n",
      "        num_rows: 7678\n",
      "    })\n",
      "})\n",
      "Distinct classes: {'neutral', 'strong positive', 'strong negative', 'moderately positive', 'positive', 'moderately negative', 'negative', 'mildly negative', 'mildly positive'}\n"
     ]
    }
   ],
   "source": [
    "# 1. Load dataset\n",
    "dataset, distinct_classes = load_and_prepare_dataset()\n",
    "\n",
    "print(dataset)\n",
    "print(\"Distinct classes:\", distinct_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "609e8828-5528-4adc-a457-c4ebcdacf4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79ad4f3431ce41bbbf2cfcd5181b571a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/996 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e850678b9a61474caebf8e3bbccd7f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f894e05bd29843aeb47a057b587a57ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35741b41beac47e3825c53c0d33b5e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded with vocab size: 32004\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c68961d9c24ee58762038d8833330c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "689981c33cbd43bf8a60335a488a2e18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3255c122e3be4a1d93d1e4dcf0eafd32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ad2730632e4dca9b5bb81f65fe1e2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb6bb3ee6b2a4d76acd6917b6db83b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b65159237cd7486c92f46033e06f3b43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d39f3a91dcc041aaabbcd18668227fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded with quantization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# 2. Load model and tokenizer\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "cache_dir = \"hfcache\"\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(model_name, cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f47396b6-2ef4-48cb-92bb-09ef99d19cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32004, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): MistralRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32004, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ada024dd-7ef3-4ff7-bc16-dad37e2d9a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='mistralai/Mistral-7B-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '<|im_end|>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<response>', '<response|end>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32000: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32001: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32002: AddedToken(\"<response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32003: AddedToken(\"<response|end>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f8ce23c-6de6-44b1-b2f0-e95a610923fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=16, target_modules={'up_proj', 'v_proj', 'o_proj', 'gate_proj', 'down_proj', 'q_proj', 'k_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, eva_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Configure LoRA\n",
    "lora_config = apply_lora_config()\n",
    "lora_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e371e6d9-c3f8-4576-b3f0-bebca25b2000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7958235c282f48f2927bd1b09d4771a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/69094 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cdbbda556124dd89f3e8da077ea87e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7678 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 69094\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 7678\n",
      "    })\n",
      "})\n",
      "Sample processed example: {'text': 'Instruction: What is the sentiment of this news? Please choose an answer from {strong negative/moderately negative/mildly negative/neutral/mildly positive/moderately positive/strong positive}.\\nInput: Yahoo Finance Live anchors Brian Sozzi, Brad Smith and Julie Hyman discuss second-quarter earnings for Amazon.\\n<response>neutral<response|end>'}\n"
     ]
    }
   ],
   "source": [
    "# 4. Preprocess the dataset\n",
    "print(\"Preprocessing dataset...\")\n",
    "\n",
    "# Preprocess dataset\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_data,\n",
    "    batched=False,\n",
    "    remove_columns=[\"instruction\", \"input\", \"output\"]\n",
    ")\n",
    "\n",
    "# Print sample for verification\n",
    "print(tokenized_dataset)\n",
    "print(\"Sample processed example:\", tokenized_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "919e27c7-7194-4075-9f4a-5252aa1aba77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response template token IDs: [32002]\n"
     ]
    }
   ],
   "source": [
    "# 5. Create data collator with completion-only loss\n",
    "response_template_ids = tokenizer.encode(RESPONSE_TEMPLATE, add_special_tokens=False)\n",
    "print(f\"Response template token IDs: {response_template_ids}\")\n",
    "\n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "    tokenizer=tokenizer,\n",
    "    response_template=RESPONSE_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eab979c",
   "metadata": {},
   "source": [
    "## Define Training Arguments\n",
    "\n",
    "Now, we will set up the training arguments, including batch size, learning rate, and other hyperparameters that control the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8b53793-8bf5-4965-a2cb-a2260ee2c24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 6. Configure training arguments\n",
    "model_output_dir = \"financial-sentiment-LoRA\"\n",
    "os.makedirs(model_output_dir, exist_ok=True)\n",
    "    \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_output_dir,\n",
    "    per_device_train_batch_size=8,         \n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,                        \n",
    "    save_strategy=\"no\",                 \n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=5,\n",
    "    max_grad_norm=1.0,                 \n",
    "    gradient_accumulation_steps=8,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    report_to=[\"wandb\"],\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    hub_private_repo=True,\n",
    "    push_to_hub=True,                    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7212323",
   "metadata": {},
   "source": [
    "## Initialize the Trainer\n",
    "\n",
    "Next, we initialize the SFTTrainer with the model, training arguments, datasets, and other configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b26e8d64-4ea8-46d8-b003-cb70b56a4259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2347/1475081074.py:2: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad18e5528b974f74a150ef785c010b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/69094 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b456b4c937d64ba895150a5bf4c92884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/69094 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486042d3ca9a4ecc9e174fc5631dfaef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/69094 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c7fa42143cd4234b5c70768ef6e5f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/69094 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d8667103cd2488e87e019c8814ceaae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting eval dataset to ChatML:   0%|          | 0/7678 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "084b3d58cef5458c954b1a2544b55ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/7678 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df012d170fda4fa7ae9070c3800c0f61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/7678 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108d7defebcb4a36b81e0d7bde61c8dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/7678 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# 7. Initialize SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,  \n",
    "    peft_config=lora_config,\n",
    "    data_collator=collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2ad1603-31df-45ad-99e5-554ee3cd7b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 41,943,040 || All params: 7,283,707,904 || Trainable%: 0.5758%\n"
     ]
    }
   ],
   "source": [
    "print_parameters(trainer.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9348b102",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b625c1d-38b8-48dd-afc2-fd77a91caddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mankitamungalpara13\u001b[0m (\u001b[33mankitamungalpara13-university-of-massachusetts-dartmouth\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20250330_153503-03qv944o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ankitamungalpara13-university-of-massachusetts-dartmouth/financial-sentiment-analysis-Mistral7B-LoRA/runs/03qv944o' target=\"_blank\">financial-sentiment-sftmodel-LoRA</a></strong> to <a href='https://wandb.ai/ankitamungalpara13-university-of-massachusetts-dartmouth/financial-sentiment-analysis-Mistral7B-LoRA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ankitamungalpara13-university-of-massachusetts-dartmouth/financial-sentiment-analysis-Mistral7B-LoRA' target=\"_blank\">https://wandb.ai/ankitamungalpara13-university-of-massachusetts-dartmouth/financial-sentiment-analysis-Mistral7B-LoRA</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ankitamungalpara13-university-of-massachusetts-dartmouth/financial-sentiment-analysis-Mistral7B-LoRA/runs/03qv944o' target=\"_blank\">https://wandb.ai/ankitamungalpara13-university-of-massachusetts-dartmouth/financial-sentiment-analysis-Mistral7B-LoRA/runs/03qv944o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1079' max='1079' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1079/1079 1:49:32, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.661800</td>\n",
       "      <td>5.774400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.472400</td>\n",
       "      <td>3.641669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f7b3eba74b3449c9177d75d5a1d0e71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5c372e57df94e16b870649eb47273c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/692M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e33ae145fb420fa06a6a12128f69b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d10331ee049451caa7784ef01ac7025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "print(\"Training the model...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model()\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0f7f675-f882-4bc2-a417-432b2d776773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar 30 17:29:23 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A6000               On  |   00000000:46:00.0 Off |                  Off |\n",
      "| 31%   41C    P8             27W /  300W |   22724MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Restart and run \n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e6550d0-c4c1-4335-807c-616daf0df785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab81b531-63e1-40fe-a847-45a0446160c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1af6a1db7744b908396528a93d7ecb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/798 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistralai/Mistral-7B-v0.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "peft_model_id = \"AnkitaMungalpara/financial-sentiment-sftmodel-LoRA\"\n",
    "device = \"cuda\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "print(config.base_model_name_or_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78d491c7-8edb-4449-9a65-06545c0a82da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<response>'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESPONSE_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61bcaeb8-1cdd-4ed9-b9f8-572565c27b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b5b3cbe0174fa2ba93a7358689ed21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfcec370ce204550ae211a0fa2c6bd4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb9af9abf714e8b9dcc41f33f5eee96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce637546683646bfbd0154cc75a93ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4032bf503854cadbe281832f97c991f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "036fbce480da4099857c7cc14995d2a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce9a5a0ecb3043238393a789429cea36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/996 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4ef685ee0b34e20ba4a323173ecb0b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94aa067046994e4db451d4e0b8e99475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db22ef8fba0d48cda4c76e33d12f17cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e9d087288c46d8ad956dda7dca4e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/692M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# When loading the base model, make sure to add the special tokens\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        config.base_model_name_or_path,\n",
    "        eos_token=\"<|im_end|>\",\n",
    "        bos_token=\"<s>\",\n",
    "        pad_token=\"<pad>\",\n",
    "        additional_special_tokens=[RESPONSE_TEMPLATE,\"<response|end>\"]\n",
    "    )\n",
    "# Resize the model's token embeddings to match the tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Now load the PEFT model\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "461cac18-0b46-40c7-9a16-eb6fdba806d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MistralForCausalLM(\n",
       "      (model): MistralModel(\n",
       "        (embed_tokens): Embedding(32004, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x MistralDecoderLayer(\n",
       "            (self_attn): MistralAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): MistralMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): MistralRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32004, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37341405-b504-4666-9433-d6b671b55e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MistralForCausalLM(\n",
       "      (model): MistralModel(\n",
       "        (embed_tokens): Embedding(32004, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x MistralDecoderLayer(\n",
       "            (self_attn): MistralAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): MistralMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): MistralRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32004, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.to(torch.float16)\n",
    "model.cuda()\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98592e53-fef4-4e3e-9ce7-03c8a1c41aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<response>'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESPONSE_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6836929c-1565-45ce-9900-e0eed6223b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID for <response|end>: 32003\n"
     ]
    }
   ],
   "source": [
    "# tokenizer.additional_special_tokens\n",
    "response_end_id = tokenizer.convert_tokens_to_ids(\"<response|end>\")\n",
    "print(f\"Token ID for <response|end>: {response_end_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d2ca49",
   "metadata": {},
   "source": [
    "## Inference: Classifying Sentiments\n",
    "\n",
    "This function generates sentiment classification responses using the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d93fb93d-d7e2-4745-9159-83606f86f352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def infer_sentiment(instruction, text, model, tokenizer, response_template=f\"{RESPONSE_TEMPLATE}\", max_new_tokens=4):\n",
    "    \"\"\"Generates sentiment classification response using fine-tuned LoRA model.\"\"\"\n",
    "    \n",
    "    # Ensure the model is on CUDA\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    input_text = f\"Instruction: {instruction}\\nInput: {text}\\n{response_template}\"\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)  # Move inputs to the same device as the model\n",
    "\n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, \n",
    "                         max_new_tokens=max_new_tokens, \n",
    "                         do_sample=True, \n",
    "                         top_p=0.99, \n",
    "                         temperature=0.001, \n",
    "                         repetition_penalty=1.1, \n",
    "                         eos_token_id=tokenizer.convert_tokens_to_ids(\"<response|end>\"))\n",
    "\n",
    "    # Decode output text\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    #print(response)\n",
    "    sentiment = response.split(response_template)[-1].strip()\n",
    "    \n",
    "    return sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "302d628d-d9d2-4955-856e-8b6020d02e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32003 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'moderately positive'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = f'What is the sentiment of this news? Please choose an answer from strong negative/moderately negative/mildly negative/neutral/mildly positive/moderately positive/strong positive.'\n",
    "text=\"Starbucks says the workers violated safety policies while workers said they'd never heard of the policy before and are alleging retaliation.\"\n",
    "\n",
    "infer_sentiment(instruction, text, model, tokenizer, response_template=\"<response>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "51efda38-c066-469b-aa1c-d60b77428c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32003 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'moderately positive'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"What is the sentiment of this news? Please choose an answer from {strong negative/moderately negative/mildly negative/neutral/mildly positive/moderately positive/strong positive\"\n",
    "text=\"Here we highlight some top-ranked technology ETFs that investors can consider betting on as a rebound rally is witnessed in tech stocks.\"\n",
    "infer_sentiment(instruction, text, model, tokenizer, response_template=\"<response>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b66a49ee-1982-40f3-a9cc-d34d186de095",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32003 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'moderately positive'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"What is the sentiment of this news? Please choose an answer from {strong negative/moderately negative/mildly negative/neutral/mildly positive/moderately positive/strong positive}.\"\n",
    "text=\"Here we highlight some top-ranked technology ETFs that investors can consider betting on as a rebound rally is witnessed in tech stocks.\"\n",
    "infer_sentiment(instruction, text, model, tokenizer, response_template=\"<response>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d95e00b-0bd8-4647-9af3-a96d28f2017f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
